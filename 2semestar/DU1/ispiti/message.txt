1. Duboki model sa sigmoidalnom izlazom, usporediti kvadratni gubitak i gubitak negativne log izglednosti
a) nađi gradijente s obzirom na ulaz sigmoide
b) koji je prikladniji i zašto
c) napiši kod u numpyju koji pronalazi gradijent (mora radit s proizvoljnim veličinama batcha)
d) koji je slučaj u kojem su gubitci isti

2. konvolucijska mreža s dvije jezgre 3x3, maxpooling 2x2 korak 2, potpuno povezani sloj i na kraju softmax
a) koja je veličina slike
b) predikcija modela ako je ulaz slike sve nule osim piksela (3,4) (3,4) (4,4) koji su postavljeni na 1 (indeksiranje kreće od 0)
c) nađi gradijente unakrsne entropije po parametrima konvolucijskog sloja i komentiraj ih ako podatak pripada klasi 0
K1 =
	[[0 0 0]
	 [0 1 0]
	 [0 0 0]] 
K2 =
	[[0 0 0]
	 [0 1 0]
	 [0 0 0]]
W =
	[[0 0 0 1 0 0 0 0]
	 [1 1 1 0 1 1 1 1]]
nema pomaka

3. Potpuno povezani sloj je zadan jednadžbama:
 	q1= w1*p1*p2 + w2*max(p1,p2)
	q2 = w1*(p1**2) + w2*(p2**2)
a) izračunaj gradijente po parametrima i po ulazima
b)implementiraj klasu kao u LAB2(metode: forward(self, inputs), backward_inputs(self, grads), backward_param(self, grads)) - može se pretpostaviti veličina grupa = 1

4.RNN sa tanh, izlaz softmax, veličina vokabulara 5000, dimenzija skrivenog sloja 500, odrediti dimenzije svih tenzora i ukupan broj parametara ako smo koristili one-hot encoding

5.RNN sa tanh, nema izlazni sloj, poznati parametri:
W_hh = 
	[[1 0]
	 [0 1]]
W_xh = 
	[[1 1]
	 [1 1]]
b = [0 0]
h0 = [0 0]
x1 = [ln(sqrt(2)) ln(sqrt(2))]
x2 = [1/5 1/5]
odrediti skriveno stanje nakon drugog koraka tj. h^2

6.sijamsko učenje, konvolucija + globalno sažimanje srednjom vrijednošću, funkcija gubitka je kvadratni kontrastni gubitak
w = [1/3 1/3 1/3], b =-0.1
xp1=[1.0 1.0 1.3 1.0 1.0 1.3], xp2=[3.3 3.3 2.7 3.3 3.3 2.7]
a) napisati jednadžbe modela i gubitka za pozitivne i negativne parove
b) provesti unaprijedni prolaz, izračunati gubitak
c) odrediti gradijente s obzirom na parametre modela


TEORIJA

1. iteracija gradijenta povećava gubitak kad:
	korak učenja prevelik, regularizacija, nikad, ...
2.LSTM rješava problem
	dugoročno pamćenje, ...
3.razlika 0-1 i unakrsne entropije
4. rano zaustavljanje i regularizacijski efekt
5.tehnike regularizacije
	povećava, smanjuje pristranost...
6.kako radi batchnorm 
7.derivacija kompozicije funkcija f(g(x))
8.što je bagging
9.numerička greška kod softmax
	kod dijeljenja, kod eksponencijalne...
10.RNN zašto pamtimo hk
11. CNN zašto radimo pooling
	dimenzionalnost smanjenje,...
12.y = a*x + b + (neka varijabla), neka varijabla je jednaka N(0,10) i uči funkcija 10. polinoma
	var i bias veliki ili mali...