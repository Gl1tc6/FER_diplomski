{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef67b941-48cd-48d2-8d5c-cf13adad0e06",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Laboratory exercise 3\n",
    "\n",
    "Required packages:\n",
    "* numpy\n",
    "* torch\n",
    "* torchvision\n",
    "* scikit-learn\n",
    "\n",
    "**Tasks that should be solved are given in the following sections:**\n",
    "* Section 5.3 The XOR problem\n",
    "* Section 5.4 The MNIST dataset\n",
    "* Section 6.1 Kernels\n",
    "* Section 6.2 Wine dataset\n",
    "* Section 6.3 Speed\n",
    "\n",
    "# Exercise 5: Multilayer perceptron\n",
    "\n",
    "The focus of this exercise will be on multilayer perceptron and to do that in a simpler way, an introduction to PyTorch is given first.\n",
    "\n",
    "## 5.1 Introduction to PyTorch\n",
    "\n",
    "We'll introduce Pytorch by quoting the [official documentation: ](https://pytorch.org/docs/stable/index.html)\n",
    "\n",
    "> \"PyTorch is an optimized tensor library for deep learning using GPUs and CPUs.\"\n",
    "\n",
    "PyTorch is [open source](https://github.com/pytorch/pytorch/tree/main#license) and [one of the most popular](https://www.kaggle.com/code/paultimothymooney/kaggle-survey-2022-all-results?scriptVersionId=107352619&cellId=45) machine learning libraries. It is simple to use and is well integrated into Python. We highly recommend that you read the [official quickstart tutorial](https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html) to further familiarize yourself with PyTorch. For any additional clarification, we suggest that you examine the [official documentation](https://pytorch.org/docs/stable/index.html). This exercise is heavily based on the mentioned documents.\n",
    "\n",
    "We'll start by importing PyTorch and [NumPy](https://numpy.org/):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156000e0-2c28-4191-85dd-a7bcd0c06980",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578ce6b4-0759-4ab9-b078-3bfb00ee555d",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### 5.1.1 PyTorch basics\n",
    "The fundamental elements of PyTorch are tensors. They are used to encode the input data, output data and the parameters of the model. Tensors are n-dimensional arrays of a given type. They are quite similar to NumPy ndarrays and we can easily integrate Numpy ndarrays into PyTorch. We can define tensors in multiple ways - we'll start by defining tensors directly from data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9772a52b-eddf-40bd-a725-b5f040c5452b",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = [[1, 2],[3, 4]]\n",
    "x_data = torch.tensor(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68c2e39-a321-4f84-a8e7-504260fa2651",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "We can list the tensors attributes with the following commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281ee2d8-bac1-4c22-8f53-51ae1158649c",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"Tensor shape    : {x_data.shape}\")\n",
    "print(f\"Tensor datatype : {x_data.dtype}\")\n",
    "print(f\"Tensor device   : {x_data.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0197365f-130d-4fda-a5bd-19102df1900d",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "We can also create tensors using functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af81a059-0a9b-4fdd-8511-f164b38ef8c3",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "shape = (2,3,)\n",
    "ones_tensor = torch.ones(shape)\n",
    "zeros_tensor = torch.zeros(shape)\n",
    "rand_tensor = torch.rand(shape) # samples from the uniform distribution on [0, 1)\n",
    "norm_tensor = torch.normal(0.0, 1.0, shape) # sample from the normal distribution with μ = 0, σ = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651e35e1-8815-4381-b48d-b27901e51cd8",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "We can also define tensors from a NumPy array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eead4b08-076f-42bf-ba1b-c8f81c0da5e1",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "np_array = np.array(data)\n",
    "x_np = torch.from_numpy(np_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67da95af-22a9-4c8e-88a2-fe63218bd881",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "And we can also define tensors using other tensors. The newly defined tensor will have the same properties as the argument tensor, unless we tell it to override some properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d15088-9bc2-4d3f-94bb-6e2b77c7750c",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('--- original tensor ----------------------------- new tensor ------------------')\n",
    "x_ones = torch.ones_like(x_data) # keeping the same properties\n",
    "print(f\"x_data shape    : {x_data.shape} \\t | x_ones shape    : {x_ones.shape}\")\n",
    "print(f\"x_data datatype : {x_data.dtype} \\t\\t | x_ones datatype : {x_ones.dtype}\")\n",
    "print(f\"x_data device   : {x_data.device} \\t\\t\\t | x_ones device   : {x_ones.device}\")\n",
    "\n",
    "print('-------------------------------------------------------------------------------')\n",
    "\n",
    "x_rand = torch.rand_like(x_data, dtype=torch.float) # overriding the datatype property\n",
    "print(f\"x_data shape    : {x_data.shape} \\t | x_rand shape    : {x_rand.shape}\")\n",
    "print(f\"x_data datatype : {x_data.dtype} \\t\\t | x_rand datatype : {x_rand.dtype}\")\n",
    "print(f\"x_data device   : {x_data.device} \\t\\t\\t | x_rand device   : {x_rand.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ae1124-49f6-4721-a513-055748c25d8b",
   "metadata": {},
   "source": [
    "### 5.1.2 Hardware acceleration\n",
    "You might wonder what does the `device` property describe? PyTorch follows a [simple and explicit design philosophy](https://pytorch.org/docs/stable/community/design.html#principle-2-simple-over-easy) and this is reflected in the way we allocate our tensors to actual hardware. By default, all tensors are allocated on the CPU and we have to manually move them to the GPU. [The official quickstart tutorial](https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html#creating-models) shows us a nice and effective method of doing this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76271498-9e24-459f-939a-73733ad76e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the device availability\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "# move the tensor to the device\n",
    "x_data = x_data.to(device)\n",
    "print(f\"x_data shape    : {x_data.shape}\")\n",
    "print(f\"x_data datatype : {x_data.dtype}\")\n",
    "print(f\"x_data device   : {x_data.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ed2aad-fbf9-47c2-93df-76d250e39bf1",
   "metadata": {},
   "source": [
    "Obviously you need to have the appropriate hardware and environment configuration in order to allow PyTorch to utilize your hardware. Depending on your environment, you will need to:\n",
    "* Local environment → install the appropriate device drivers\n",
    "  * depends on your operating system and the device you're using\n",
    "  * you will also need to install the PyTorch version that supports your hardware\n",
    "* Google Collab → enable hardware acceleration\n",
    "  * select `Runtime > Change runtime type`, then choose the appropriate hardware (ex. `T4 GPU`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38a9f37-c1ee-4734-8ea7-b4bac2824100",
   "metadata": {},
   "source": [
    "### 5.1.3 Tensor operations\n",
    "PyTorch provides [a large number of tensor operations](https://pytorch.org/docs/stable/torch.html) that can be hardware accelerated. If you're familiar with NumPy, using these operations will feel quite natural. First let's try some arithmetic operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3607d2e-22b6-47d8-ba70-e8139d6e13a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = torch.rand_like(rand_tensor)\n",
    "\n",
    "add_tensor = tensor + tensor            # equivalent to torch.add(tensor, tensor)\n",
    "mul_tensor = tensor * 10                # equivalent to torch.mul(tensor, 10)\n",
    "\n",
    "trans_tensor = tensor.T                 # equivalent to torch.t*tensor)\n",
    "element_wise_product = tensor * tensor  # equivalent to torch.mul(tensor, tensor)\n",
    "matrix_product = tensor @ tensor.T      # equivalent to torch.matmul(tensor, tensor.T)\n",
    "\n",
    "print(\"starting tensor...\", tensor, sep='\\n')\n",
    "print(\"added tensor...\", add_tensor, sep='\\n')\n",
    "print(\"multiplied tensor...\", mul_tensor, sep='\\n')\n",
    "print(\"transposed tensor...\", trans_tensor, sep='\\n')\n",
    "print(\"element-wise product tensor...\", element_wise_product, sep='\\n')\n",
    "print(\"matrix product tensor...\", matrix_product, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed8d0cd-f534-40b0-89e3-97d77c505946",
   "metadata": {},
   "source": [
    "PyTorch tensors can be indexed and sliced in a [similar way to NumPy ndarrays](https://numpy.org/devdocs/user/quickstart.html#indexing-slicing-and-iterating):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93475f33-f290-4f48-9936-62f7ad59bca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = torch.rand(5, 5)\n",
    "second_row = tensor[1]\n",
    "third_collumn = tensor[:, 2]\n",
    "second_to_last_collumn = tensor[..., -2]\n",
    "assign_tesnor = torch.ones_like(tensor)\n",
    "assign_tesnor[1:3, 1:3] = 10 # multiple element assignment\n",
    "\n",
    "print(\"starting tensor...\", tensor, sep='\\n')\n",
    "print(\"second_row ...\", second_row, sep='\\n')\n",
    "print(\"third collumn ...\", third_collumn, sep='\\n')\n",
    "print(\"second to last collumng...\", second_to_last_collumn, sep='\\n')\n",
    "print(\"assignment tensor...\", assign_tesnor, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bbe981b-8fb4-4dad-8a5d-26090c236df5",
   "metadata": {},
   "source": [
    "Operations with the `_` suffix denote the in-place operations. They will store the resulting tensor into the argument tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e4d1a7-940a-4d1c-b89f-893ec58c0a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "assign_tesnor.mul_(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2960bc31-691f-4bd1-b325-7e0ae7579913",
   "metadata": {},
   "source": [
    "### 5.1.4 Datasets and dataloaders\n",
    "Even though we can store our data directly into tensors and work directly with them, a preferred method for working with data is to use the `Dataset` and `Dataloader` objects. This will also allow us to decouple our data loading code from the model training code. In order to load our data, we have to create a new class that extends the `Dataset` class. We also need to implement the following methods:\n",
    "\n",
    "* `__init__` - initializes the `Dataset` object\n",
    "* `__len__` - returns the number of samples in our object\n",
    "* `__getitem__` - returns the sample for a given index *(remember to return the (input, output) pair)*\n",
    "\n",
    "Let's generate a synthetic dataset that we'll later use for our simple linear regression model. Our dataset will have 100 samples. The input will be a 3-dimensional tensor, and the output will be a 1-dimensional tensor. The inputs will be normally distributed ($\\mu = 0$, $\\sigma = 1$) and our model will be defined with the following expression : $y=\\mathbf{w}^{T}\\mathbf{x}+\\mathbf{b}$.\n",
    "\n",
    "We'll start by defining our custom dataset class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5748125b-7011-4f42-adc3-9abdf062f55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomDatasetAlpha(Dataset):                                    # extending the Dataset class ...\n",
    "    def __init__(self, number_of_samples, weights, bias):             # our dataset is defined by:\n",
    "        self.number_of_samples = number_of_samples                    #  - the number of samples\n",
    "        self.weights = weights                                        #  - the weights vector\n",
    "        self.bias = bias                                              #  - the bias vector\n",
    "        \n",
    "        self.x_input = torch.normal(0.0, 1.0, (number_of_samples, 3)) # sampling our inputs\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.number_of_samples\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # we're assuming that our input and weights are\n",
    "        # represented as row vectors. Therefore, we'll \n",
    "        # calculate the dot product as such:\n",
    "        input = self.x_input[index]\n",
    "        output = self.x_input[index] @ self.weights.T + self.bias     # the model expression\n",
    "        return input, output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e973ac-e949-464d-bcf9-5422f87b7b03",
   "metadata": {},
   "source": [
    "After we've defined our dataset, we can instantiate it with the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459085ab-8a8f-4f05-bc37-8bfa38c687d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = torch.tensor([9.0, 8.0, 7.0])\n",
    "bias = torch.tensor([5.0])\n",
    "dataset = CustomDatasetAlpha(100, weights, bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a47a39e-831e-4880-b366-3c7b104f0c38",
   "metadata": {},
   "source": [
    "We can access our data by simply indexing our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f73f0d-9865-4e1c-ae1a-da8747f11c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"6th samples of the dataset\")\n",
    "print(dataset[5])\n",
    "\n",
    "print(\"last 5 samples of the dataset\")\n",
    "print(dataset[-6:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62eb917-46c2-4167-9b66-1ab65de8f2ed",
   "metadata": {},
   "source": [
    "With our `Dataset` object we can access our samples directly. We could implement additional methods for obtaining multiple samples, data shuffling,... but instead we can use the useful `DataLoader` class that will abstract this complexity for us. This can be done with the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77078ea1-3d05-4f1c-b991-16270bb71127",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "# iterating trough the DataLoader\n",
    "for batch_x, batch_y in dataloader:\n",
    "    print(\"current batch inputs :\", batch_x, sep='\\n')\n",
    "    print(\"current batch outputs :\", batch_y, sep='\\n')\n",
    "    \n",
    "    # we'll break after the first iteration in order to\n",
    "    # minimize the printed output\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9f8bba-b9df-4fe6-96d7-f2b42d2f3db5",
   "metadata": {},
   "source": [
    "If you re-run the previous cell you should see different samples in our batch, since we've set the `shuffle` flag to be true. Hopefully this demonstrates the usefulness of the `DataLoader` class. Feel free to explore [other flags](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) and other [data loading utilities](https://pytorch.org/docs/stable/data.html#module-torch.utils.data).\n",
    "\n",
    "When it comes to loading images, PyTorch provides an additional library named `torchvision` that contains a lot helpful functions for working with images. It also contains a lot of popular datasets and model architectures that are commonly used in computer vision. If you're interested in loading your own images, check out the official [Datasets & DataLoaders tutorial](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html#creating-a-custom-dataset-for-your-files). We'll use `torchvision` later on in this exercise in order to load [the MNIST dataset](http://yann.lecun.com/exdb/mnist/), a quite popular image dataset for testing our machine learning models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa63ba5-522a-4b88-9177-33ad049c09c3",
   "metadata": {},
   "source": [
    "### 5.1.5 Models\n",
    "Now we'll discuss how to create our models using PyTorch. All models in PyTorch extend the `nn.Module` class, similarly how datasets extend the `Dataset` class. During the initialization of the model, we'll define our network's architecture. We'll also define the `forward` method that will calculate the forward pass of our network. Let's start by defining a simple linear regression model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61cbe5a7-ad20-4044-aaaa-dda1067c2e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class NeuralNetworkAlpha(nn.Module):          # extending the nn.Module...\n",
    "    def __init__(self):                       # we won't need any additional parameters\n",
    "        super().__init__()                    # calling the constructor of the parent class\n",
    "        self.linear_layer = nn.Linear(3, 1)   # linear layer with input size of 3, output size of 1\n",
    "                                              # to model linear regression\n",
    "\n",
    "    def forward(self, x):                     # defining the forward pass\n",
    "        return self.linear_layer(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1822e26c-8f35-45a4-944e-c91daaea1656",
   "metadata": {},
   "source": [
    "Recall that a single layered neural network with linear activation models linear regression. PyTorch provides [a large ammount of layers](https://pytorch.org/docs/stable/nn.html) and we will give a list of commonly used ones:\n",
    "* [`Linear`](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear) - defines a layer with linear activation function\n",
    "* [`ReLU`](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html#torch.nn.ReLU) - defines a layer with ReLu activation function\n",
    "* [`Sigmoid`](https://pytorch.org/docs/stable/generated/torch.nn.Sigmoid.html#torch.nn.Sigmoid) - defines a layer with sigmoid activation function\n",
    "* [`Conv2d`](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d) - defines a 2D convolutional layer\n",
    "* [`Sequential`](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html#torch.nn.Sequential) - defines a sequential container which can take other layers and will apply them sequentially\n",
    "* [`Flatten`](https://pytorch.org/docs/stable/generated/torch.nn.Flatten.html#torch.nn.Flatten) - defines a layer that will flatten a multiple dimensional tensor into a single dimension\n",
    "\n",
    "We'll use some of these layers later on in this exercise. Let's continue by instantiating our network and calculating the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b256c9e-8468-4a47-aaf9-7a18a7cd8143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we create our network and move it to previously defined device\n",
    "network = NeuralNetworkAlpha().to(device)\n",
    "print(\"structure of our network...\", network, sep='\\n')\n",
    "\n",
    "print(\"first dataset sample...\")\n",
    "print(dataset[0][0])\n",
    "print(\"target...\")\n",
    "print(dataset[0][1])\n",
    "print(\"network's output for the first dataset sample...\")\n",
    "print(network(dataset[0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2346f00-a518-49cc-bbd9-d4be922c8247",
   "metadata": {},
   "source": [
    "We can inspect the network's parameters with the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad369cf-49f1-4cd9-9c3f-39fb7f2429a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in network.named_parameters():\n",
    "    print(name, param)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db19f30-a687-4ade-8e01-39b5be4e7e42",
   "metadata": {},
   "source": [
    "We can see the values of parameters for each layer *(if you want a small exercise, manually verify that the output of our network was correct)*. Let's proceed with this exercise and see how we can use PyTorch to train our network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07e4b18-bf0e-4ed1-8be1-124be3211d11",
   "metadata": {},
   "source": [
    "### 5.1.6 Parameter optimization\n",
    "In order to optimize our network using back propagation, we'll need to compute the gradients of the loss function with respect towards each parameter. PyTorch allows us to achieve this with automatic differentiation using `torch.autograd`, it's built-in differentiation engine. If we look back at the previous code cell, we can see that the layers of the network are tensors which have the property `requires_grad` equal to true. This makes the tensor's gradients available to be computed and allows PyTorch to calculate the backward pass for our network. If you want to learn more about the details of gradient calculation, check out the official [Autograd tutorial](https://pytorch.org/tutorials/beginner/basics/autogradqs_tutorial.html).\n",
    "\n",
    "Let's use the [mean squared error](https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html#torch.nn.MSELoss) as our loss function and compute the gradients for the first dataset sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a2d9e2-0c48-419d-b42a-97d8513898df",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "first_sample = dataset[0]\n",
    "\n",
    "first_prediction = network(first_sample[0])              # forward\n",
    "first_loss = loss_fn(first_sample[1], first_prediction)   # pass\n",
    "\n",
    "print(\"first sample...\", first_sample, sep='\\n')\n",
    "print(\"first prediction...\", first_prediction, sep='\\n')\n",
    "print(\"MSE loss : \", first_loss)\n",
    "\n",
    "first_loss.backward()                                   # backward pass\n",
    "\n",
    "for name, param in network.named_parameters():\n",
    "    print(\"gradient of : \", name, param.grad)          # printing the gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8aa82f-fa81-4c20-bc77-d920bc015115",
   "metadata": {},
   "source": [
    "We can see that our network's parameters are stored in the `param.grad` attribute. It is important to see how we've used the loss function:\n",
    "1) `loss_fn` - what our loss function is\n",
    "2) `first_loss` - represents the loss for given network predictions and ground truths\n",
    "3) `first_loss.backward` - propagating the gradients to our parameters\n",
    "\n",
    "Hopefully this demonstrates how PyTorch calculates and propagates the gradients of our loss function. \n",
    "\n",
    "Now we need to apply these gradients to our parameters. This is done with [an `Optimizer`](https://pytorch.org/docs/stable/optim.html). When defining an optimizer we need to provide it with parameters that should be optimized. In our case, these are the network's parameters. PyTorch provides [many optimization algorithms](https://pytorch.org/docs/stable/optim.html#algorithms) that apply gradients to our parameters. We'll use [stochastic gradient descent](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html#torch.optim.SGD) as our optimizer. Let's instantiate our network, redefine our data loader, initialize the optimizer and define our hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195904a3-d370-4003-9d54-3dda9fc7ab6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import SGD\n",
    "\n",
    "# defining the hyperparameters\n",
    "epochs, learning_rate, batch_size = 100, 0.1, 100\n",
    "\n",
    "# redefining the dataloader with the new batch size\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle = True)\n",
    "\n",
    "# instantiating our network\n",
    "network = NeuralNetworkAlpha().to(device)\n",
    "\n",
    "# defining our optimizer for our network's parameters\n",
    "optimizer = SGD(network.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c94050-a44e-4366-8d36-349777633479",
   "metadata": {},
   "source": [
    "Finally we need to iterate over our whole dataset and repeat the whole process - this is our training loop. Let's proceed with defining our training loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45bb05d3-ffa9-4f83-93b0-0eb547498f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    # set the network to training mode\n",
    "    network.train()\n",
    "\n",
    "    epoch_loss = 0\n",
    "    number_of_samples = len(dataloader.dataset)\n",
    "    \n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # forward pass\n",
    "        preds = network(X)   \n",
    "        loss = loss_fn(y, preds)\n",
    "        \n",
    "        \n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # reseting the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # updating the epoch loss with the current batch loss\n",
    "        epoch_loss += loss\n",
    "    \n",
    "    # printing the loss value for every 10th epoch\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch} average loss : {epoch_loss/number_of_samples}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d1edeb-416b-44b1-bfcf-5305e0dd28ef",
   "metadata": {},
   "source": [
    "We can see that the value of the loss function is decreasing, meaning that our network's parameters are being optimized for our training dataset. If we inspect the parameter values of our network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636bfdf1-d688-4866-b017-9a3d1d19812d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in network.named_parameters():\n",
    "    print(name, param)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41020072-a3d0-41c5-a211-47071e91fd0e",
   "metadata": {},
   "source": [
    "We can see that we've achieved the same values that we used for our synthetic dataset.\n",
    "\n",
    "You might wonder why we've called `network.train()` inside our training loop? This is a best practice according to [the official documentation](https://pytorch.org/docs/stable/notes/autograd.html#evaluation-mode-nn-module-eval) :\n",
    "> It is recommended that you always use `model.train()` when training and `model.eval()` when evaluating your model (validation/testing) even if you aren’t sure your model has training-mode specific behavior, because a module you are using might be updated to behave differently in training and eval modes.\n",
    "\n",
    "This concludes our introduction to PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc69d9a1-3ea2-4f90-b84b-313fa238e025",
   "metadata": {},
   "source": [
    "## 5.3 The XOR problem\n",
    "\n",
    "XOR samples are not linearly separable. However, they can be separated by introducing non-linearities with our activation function. We'll start by defining our dataset using a helpful wrapper [TensorDataset](https://pytorch.org/docs/stable/data.html#torch.utils.data.TensorDataset):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80155416-60f7-43f5-98ee-e94a972903a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataset import TensorDataset\n",
    "\n",
    "xor_in = torch.tensor([\n",
    "    [0, 0],\n",
    "    [0, 1],\n",
    "    [1, 0],\n",
    "    [1, 1]], dtype = torch.float)\n",
    "\n",
    "xor_out = torch.tensor([0, 1, 1, 0], dtype = torch.float) # it is important to use the correct datatype\n",
    "xor_dataset = TensorDataset(xor_in, xor_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e292821d-be45-4425-b11c-c7491dc8d179",
   "metadata": {},
   "source": [
    "Now we'll define our network using a [Sequential container](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html#sequential) for our layers. Our network is also parameterized by a given activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa4c26a-6296-40c7-8a9a-1efc28e950b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class XORNetwork(nn.Module):\n",
    "    def __init__(self, activation_func):\n",
    "        super().__init__()\n",
    "        self.layer_stack = nn.Sequential(\n",
    "            nn.Linear(2, 2),\n",
    "            activation_func(),\n",
    "            nn.Linear(2, 1)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layer_stack(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d23429-9e23-4488-92f1-585591dd5f8b",
   "metadata": {},
   "source": [
    "Let's instantiate our network, redefine our data loader and loss function, initialize the optimizer and define our hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e084138f-af7a-45a4-98a9-442d9638e011",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "# defining the hyperparameters\n",
    "epochs, learning_rate, batch_size = 10000, 0.1, 1\n",
    "\n",
    "# defining the dataloader\n",
    "xor_dataloader = DataLoader(xor_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# defining the activation function\n",
    "activation_func = nn.Tanh\n",
    "\n",
    "# instantiating the network\n",
    "xor_network = XORNetwork(activation_func).to(device)\n",
    "\n",
    "# defining the optimizer type\n",
    "optimizer_type = Adam\n",
    "\n",
    "# defining our optimizer for our network's parameters\n",
    "optimizer = optimizer_type(xor_network.parameters(), lr = learning_rate)\n",
    "\n",
    "# redefining our loss function\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# defining the threshold for stopping the training\n",
    "threshold=1e-6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2806938d-1ccf-40d1-a6a6-864b42cd2ebf",
   "metadata": {},
   "source": [
    "In practice, we recommend that you define a general training loop function instead of retyping the training loop for each network. [The official tutorial](https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html#full-implementation) provides a great example. In this exercise we're going to repeat our training loop for explanatory purposes. Let's proceed with our training loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be263e5-a28e-4d7b-a37c-514ec8e5182c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    # set the network to training mode\n",
    "    xor_network.train()\n",
    "\n",
    "    epoch_loss = 0\n",
    "    number_of_samples = len(xor_dataloader.dataset)\n",
    "    \n",
    "    for batch, (X, y) in enumerate(xor_dataloader):\n",
    "        # forward pass\n",
    "        preds = xor_network(X)\n",
    "        y = y.unsqueeze(1)\n",
    "        \n",
    "        loss = loss_fn(y, preds)\n",
    "\n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # reseting the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # updating the epoch loss with the current batch loss\n",
    "        epoch_loss += loss\n",
    "    \n",
    "    # printing the loss value for every 100th epoch\n",
    "    if (epoch+1) % 100 == 0:\n",
    "        print(f\"Epoch {epoch} average loss : {epoch_loss/number_of_samples}\")\n",
    "    if (epoch_loss/number_of_samples < threshold):\n",
    "        print(f\"Epoch {epoch} average loss : {epoch_loss/number_of_samples}\")\n",
    "        print(f\"Training is finished after epoch {epoch+1}\")\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d074d1df-43e9-42ed-97a0-2618fbaa380c",
   "metadata": {},
   "source": [
    "**Task**\n",
    "\n",
    "1. How many epochs are required for the training process to converge for each combination of the chosen activation function (Sigmoid, Tanh), optimizer (SGD/Adam)  and various learning rates (0.1, 0.01)? Display the results using the table with the following headers: **Activation function, Optimizer, Learning rate, Number of epochs**).\n",
    "2. Which combination turned out to be the best? Are there combinations for which the training process does not converge?\n",
    "\n",
    "*Remember to re-evaluate the previous two cells in order to properly apply changes. *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd29ef7-b8b4-400c-8a88-1b14e024a427",
   "metadata": {},
   "source": [
    "## 5.4 The MNIST dataset\n",
    "\n",
    "[The MNIST dataset](http://yann.lecun.com/exdb/mnist/) is a very popular dataset which contains 60,000 training and 10,000 test images of handwritten digits. It is used to test the ability of a method to recognize which digit is on a given image. MNIST dataset is included with the [`torchvision` library](https://pytorch.org/vision/stable/index.html), so let's start by importing the train and test datasets:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb767eb-14fe-4e02-aa77-eeadd7eea913",
   "metadata": {},
   "source": [
    "We've also used `ToTensor` function to transform MNIST images to tensors. Transforms are PyTorch mechanism of data processing. For additional details, check out [the Transforms tutorial](https://pytorch.org/tutorials/beginner/basics/transforms_tutorial.html).\n",
    "\n",
    "Let's continue by defining our network. We're going to ignore the spatial distribution of individual image pixels and simply use individual pixels as features. This can simply be done by [flattening our images](https://pytorch.org/docs/stable/generated/torch.nn.Flatten.html#torch.nn.Flatten). We'll also add aditional hidden layers with activation functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ffceea-3e03-422a-80e9-66e23aced46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import MNIST\n",
    "from torchvision.datasets import FashionMNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "# download MNIST from the internet and store it in the notebook's directory\n",
    "mnist_train = MNIST(root = '', train = True, download = True, transform = ToTensor())\n",
    "mnist_test = MNIST(root = '', train = False, download = True, transform = ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a899cea4-a96b-4554-8452-17b091e971bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTNetwork(nn.Module):\n",
    "    def __init__(self, hidden_layer_size1, hidden_layer_size2, activation_funct):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        # 28x28 is the resolution of the images\n",
    "        self.pixel_count = 28 * 28\n",
    "        self.number_of_classes = 10\n",
    "        \n",
    "        self.layer_stack = nn.Sequential(\n",
    "            nn.Linear(self.pixel_count, hidden_layer_size1),\n",
    "            activation_func(),\n",
    "            nn.Linear(hidden_layer_size1, hidden_layer_size2),\n",
    "            activation_func(),\n",
    "            nn.Linear(hidden_layer_size2, self.number_of_classes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        return self.layer_stack(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7854f313-45bf-4e79-90a4-c1cd07431683",
   "metadata": {},
   "source": [
    "Let's instantiate all of the needed classes and variables. Since we're doing classification, we're going to use [cross entropy loss function](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf93d8f7-8eb4-4ff5-a9fb-fd871ccceb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the hyperparameters\n",
    "epochs, learning_rate, batch_size = 20, 0.01, 100\n",
    "\n",
    "# defining the dataloaders\n",
    "mnist_train_dataloader = DataLoader(mnist_train, batch_size=batch_size, shuffle=True)\n",
    "mnist_test_dataloader = DataLoader(mnist_test, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# defining the activation function\n",
    "# activation_func = nn.Sigmoid\n",
    "# activation_func = nn.Tanh\n",
    "activation_func = nn.ReLU\n",
    "\n",
    "# instantiating the network\n",
    "hidden_layer_size_1 = 256\n",
    "hidden_layer_size_2 = 256\n",
    "mnist_network = MNISTNetwork(hidden_layer_size_1, hidden_layer_size_2, activation_func).to(device)\n",
    "\n",
    "# defining the optimizer type\n",
    "# optimizer_type = SGD\n",
    "optimizer_type = Adam\n",
    "\n",
    "# defining our optimizer for our network's parameters\n",
    "optimizer = optimizer_type(mnist_network.parameters(), lr = learning_rate)\n",
    "\n",
    "# redefining our loss function\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3073774-3e8b-422f-9cb9-d1fe88fb2977",
   "metadata": {},
   "source": [
    "And let's run our training loop. We're also going to print the print the value of the loss function for the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68bfd85-1f51-47f6-8676-87593d3dfcf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    # set the network to training mode\n",
    "    mnist_network.train()\n",
    "\n",
    "    epoch_loss = 0\n",
    "    number_of_samples = len(mnist_train_dataloader.dataset)\n",
    "        \n",
    "    for batch, (X, y) in enumerate(mnist_train_dataloader):\n",
    "        # forward pass\n",
    "        preds = mnist_network(X)\n",
    "        loss = loss_fn(preds, y)\n",
    "\n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # reseting the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # updating the epoch loss with the current batch loss\n",
    "        epoch_loss += loss\n",
    "\n",
    "    # printing the train loss value for every n epochs\n",
    "    if (epoch+1) % 1 == 0:\n",
    "        print(f\"Epoch {epoch} average train loss : {epoch_loss/number_of_samples}\")\n",
    "\n",
    "    # printing the test set metrics for every n epochs\n",
    "    if (epoch+1) % 1 == 0:\n",
    "        test_loss = 0\n",
    "        correct = 0\n",
    "        number_of_test_samples = len(mnist_test_dataloader.dataset)\n",
    "\n",
    "        # set the network to evaluation mode\n",
    "        mnist_network.eval()\n",
    "        \n",
    "        for (X, y) in mnist_test_dataloader:\n",
    "            preds = mnist_network(X)\n",
    "            loss = loss_fn(preds, y)\n",
    "\n",
    "            # updating the test loss with the current batch loss\n",
    "            test_loss += loss\n",
    "\n",
    "            # updating the number of correctly classified images\n",
    "            # this great one-liner was copied from \n",
    "            # https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html#full-implementation\n",
    "            correct += (preds.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "        print(f\"Epoch {epoch} average test loss : {test_loss/number_of_test_samples}\")\n",
    "        print(f\"Epoch {epoch} test accuracy : {correct/number_of_test_samples}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca51440-2dab-4c55-bccb-eb3a53d18a84",
   "metadata": {},
   "source": [
    "Notice that when we evaluated the network on the test set, we've set the network to the evaluation mode using `.eval()` method. This is again best practice according to [the official documentation](https://pytorch.org/docs/stable/notes/autograd.html#evaluation-mode-nn-module-eval).\n",
    "\n",
    "**Task**\n",
    "\n",
    "1. Experiment with different activation functions, learning rates, batch sizes, optimizers, and architectures. \n",
    "   * What is the best combination of them? \n",
    "   * Which of them has the highest impact on the accuracy and rate of convergence? \n",
    "   * How about the size of hidden layers? Make the comparisons and draw the appropriate plots.\n",
    "\n",
    "*Remember to re-evaluate the previous two cells in order to properly apply changes.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e32f8a",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 6 Support Vector Machine\n",
    "## 6.1 Kernels\n",
    "Support Vector Machine can [use different kernels](https://en.wikipedia.org/wiki/Kernel_method): linear, radial basis function, polynomial, sigmoid, etc. The difference between some of them can be seen after running the code below that uses a classical example. Besides the usual packages, the *sklearn* package is also used here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915b4bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm, datasets\n",
    "\n",
    "#take the well-known iris dataset\n",
    "iris = datasets.load_iris()\n",
    "#we will use only sepal length and width\n",
    "features=iris.data[:, :2]\n",
    "classes=iris.target\n",
    "\n",
    "#plot points\n",
    "x1, x2=features[:, 0], features[:, 1]\n",
    "x_min, x_max=x1.min()-1, x1.max()+1\n",
    "y_min, y_max=x2.min()-1, x2.max()+1\n",
    "h=0.02\n",
    "plot_x, plot_y=np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "#regularization\n",
    "C=1.0  \n",
    "models=(svm.SVC(kernel=\"linear\", C=C),\n",
    "          svm.SVC(kernel=\"rbf\", gamma=0.7, C=C),\n",
    "          svm.SVC(kernel=\"poly\", degree=3, C=C))\n",
    "models=(model.fit(features, classes) for model in models)\n",
    "\n",
    "# title for the plots\n",
    "titles = (\"Linear kernel\", \"RBF kernel\", \"Polynomial (degree 3) kernel\")\n",
    "\n",
    "\n",
    "for model, title in zip(models, titles):\n",
    "    points=model.predict(np.c_[plot_x.ravel(), plot_y.ravel()]).reshape(plot_x.shape)\n",
    "    plt.contourf(plot_x, plot_y, points, cmap=plt.cm.coolwarm, alpha=0.8)\n",
    "    plt.xlim(plot_x.min(), plot_x.max())\n",
    "    plt.ylim(plot_y.min(), plot_y.max())\n",
    "    plt.xlabel(\"Sepal length\")\n",
    "    plt.ylabel(\"Sepal width\")\n",
    "    plt.title(title)\n",
    "    \n",
    "    predicted=model.predict(features);\n",
    "    print(\"Accuracy: %.2lf%%\"%(100*np.sum(classes==predicted)/classes.size))\n",
    "    \n",
    "    plt.scatter(x1, x2, c=classes, cmap=plt.cm.coolwarm, s=20, edgecolors=\"k\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71810e65",
   "metadata": {},
   "source": [
    "**Tasks**\n",
    "\n",
    "1. What accuracies are achieved when other two features of the iris dataset are used?\n",
    "2. Split the iris dataset into a training and testing part with the ratio 70/30, fit the SVM model with different kernels on the training part, and test it on the testing part. Which kernel gives the highest accuracy? Take care of the target class distribution in the training/testing part.\n",
    "3. Make the code below give over 90% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4086458b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import svm, datasets\n",
    "\n",
    "n1=400\n",
    "n2=400\n",
    "\n",
    "class1=(np.tile(np.random.uniform(low=0.0, high=1, size=n2).reshape((n2, 1)), (1, 2))+3/2)*\\\n",
    "np.array([(np.cos(a), np.sin(a)) for a in np.random.uniform(low=2, high=8, size=n2)])+np.tile(np.array([[3/2, 0]]), (n1, 1))\n",
    "\n",
    "class2=(np.tile(np.random.uniform(low=0.0, high=1, size=n2).reshape((n2, 1)), (1, 2))+3/2)*\\\n",
    "np.array([(np.cos(a), np.sin(a)) for a in np.random.uniform(low=-1, high=4, size=n2)])\n",
    "x=np.vstack((class1, class2))\n",
    "y=np.concatenate((np.ones((n1)), 2*np.ones((n2))))\n",
    "\n",
    "idx=np.random.permutation(y.size)\n",
    "\n",
    "x=x[idx, :]\n",
    "y=y[idx]\n",
    "\n",
    "s=round((n1+n2)/2)\n",
    "\n",
    "x_train=x[:s, :]\n",
    "y_train=y[:s]\n",
    "\n",
    "x_test=x[s:, :]\n",
    "y_test=y[s:]\n",
    "\n",
    "#EDIT from ....\n",
    "model= svm.SVC(kernel=\"linear\")\n",
    "model.fit(x_train, y_train)\n",
    "#...TO HERE\n",
    "\n",
    "predicted=model.predict(x_test);\n",
    "print(\"Accuracy: %.2lf%%\"%(100*np.sum(y_test==predicted)/y_test.size))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9d129e",
   "metadata": {},
   "source": [
    "## 6.2 Wine dataset\n",
    "Here we are going to make some experiments with the wine dataset to see how features can [affect](https://en.wikipedia.org/wiki/Feature_selection) the classification.\n",
    "\n",
    "**Tasks**\n",
    "\n",
    "1. Which SVM kernel will achieve the highest accuracy when all features are used?\n",
    "2. If you can use **only one** feature and any kernel to achieve highest possible accuracy, which feature and kernel would that be?\n",
    "3. If you can use **only two** features and any kernel to achieve highest possible accuracy, which feature and kernel would that be?\n",
    "4. How do you explain the results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ca5e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "wine=load_wine()\n",
    "x=wine.data\n",
    "y=wine.target\n",
    "idx=np.random.permutation(y.size)\n",
    "x=x[idx, :]\n",
    "y=y[idx]\n",
    "\n",
    "#all features\n",
    "features_idx=range(x.shape[1])\n",
    "#only some of the features\n",
    "#features_idx=[0, 1]\n",
    "\n",
    "x=x[:, features_idx]\n",
    "\n",
    "s=round(y.size/2)\n",
    "\n",
    "x_train=x[:s, :]\n",
    "y_train=y[:s]\n",
    "\n",
    "x_test=x[s:, :]\n",
    "y_test=y[s:]\n",
    "\n",
    "model=svm.SVC()\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "predicted=model.predict(x_test)\n",
    "print(\"Accuracy: %.2lf%%\"%(100*np.sum(y_test==predicted)/y_test.size))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7d2025",
   "metadata": {},
   "source": [
    "## 6.3 Speed\n",
    "SVM is really great, but it has an important disadvantage with respect to neural networks in general. Here we are going to demonstrate it.\n",
    "\n",
    "**Tasks**\n",
    "1. Run the code below for various dataset sizes and each time store the time needed for the model to fit.\n",
    "2. Draw a plot that shows the influence of dataset size on execution time.\n",
    "3. How would you model the influence?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40cf8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import svm, datasets\n",
    "\n",
    "def create_data(n1, n2):\n",
    "    class1=np.c_[np.random.normal(0, 1, size=n1), np.random.normal(0, 1, size=n1)]\n",
    "    class2=np.c_[np.random.normal(2, 1, size=n2), np.random.normal(0, 1, size=n2)]\n",
    "    x=np.vstack((class1, class2))\n",
    "    y=np.concatenate((np.ones((n1)), 2*np.ones((n2))))\n",
    "    \n",
    "    return x, y\n",
    "\n",
    "x, y=create_data(5000, 5000)\n",
    "\n",
    "import time;\n",
    "start=time.time()\n",
    "model.fit(x, y)\n",
    "end=time.time();\n",
    "t=end-start\n",
    "print(t)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
